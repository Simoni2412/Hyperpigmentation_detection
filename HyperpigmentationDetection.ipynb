{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlCyALQ6v-SM"
      },
      "source": [
        "# Hyperpigmentation Detection System\n",
        "\n",
        "This notebook implements a comprehensive hyperpigmentation detection system using deep learning models. The system processes original facial images and green-annotated hyperpigmentation masks to train various segmentation models.\n",
        "\n",
        "## Features:\n",
        "- Multiple preprocessing techniques for facial images\n",
        "- Green color segmentation for annotation masks\n",
        "- Multiple model architectures (U-Net, DeepLabV3+, SegNet)\n",
        "- Comprehensive evaluation metrics\n",
        "- Real-time inference pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9auR0ow1v-SO",
        "outputId": "0d897319-452d-42de-f145-90ef490b47b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-contrib-python-headless==4.8.0.74 in /usr/local/lib/python3.12/dist-packages (4.8.0.74)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from opencv-contrib-python-headless==4.8.0.74) (1.26.4)\n",
            "Collecting mediapipe==0.10.15\n",
            "  Using cached mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting absl-py (from mediapipe==0.10.15)\n",
            "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting attrs>=19.1.0 (from mediapipe==0.10.15)\n",
            "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting flatbuffers>=2.0 (from mediapipe==0.10.15)\n",
            "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting jax (from mediapipe==0.10.15)\n",
            "  Using cached jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe==0.10.15)\n",
            "  Using cached jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting matplotlib (from mediapipe==0.10.15)\n",
            "  Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy<2 (from mediapipe==0.10.15)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting opencv-contrib-python (from mediapipe==0.10.15)\n",
            "  Using cached opencv_contrib_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.15)\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.15)\n",
            "  Using cached sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe==0.10.15)\n",
            "  Using cached cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax->mediapipe==0.10.15)\n",
            "  Using cached ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax (from mediapipe==0.10.15)\n",
            "  Using cached jax-0.7.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe==0.10.15)\n",
            "  Using cached jaxlib-0.7.2-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe==0.10.15)\n",
            "  Using cached jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe==0.10.15)\n",
            "  Using cached jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting opt_einsum (from jax->mediapipe==0.10.15)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting scipy>=1.12 (from jax->mediapipe==0.10.15)\n",
            "  Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->mediapipe==0.10.15)\n",
            "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->mediapipe==0.10.15)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->mediapipe==0.10.15)\n",
            "  Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->mediapipe==0.10.15)\n",
            "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib->mediapipe==0.10.15)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pillow>=8 (from matplotlib->mediapipe==0.10.15)\n",
            "  Using cached pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting pyparsing>=3 (from matplotlib->mediapipe==0.10.15)\n",
            "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->mediapipe==0.10.15)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe==0.10.15)\n",
            "  Using cached opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.15)\n",
            "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.15)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Using cached sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Using cached jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
            "Using cached jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n",
            "Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "Using cached opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "Using cached cffi-2.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (219 kB)\n",
            "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
            "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "Using cached ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Installing collected packages: flatbuffers, six, pyparsing, pycparser, protobuf, pillow, packaging, opt_einsum, numpy, kiwisolver, fonttools, cycler, attrs, absl-py, scipy, python-dateutil, opencv-contrib-python, ml_dtypes, contourpy, CFFI, sounddevice, matplotlib, jaxlib, jax, mediapipe\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.9.23\n",
            "    Uninstalling flatbuffers-25.9.23:\n",
            "      Successfully uninstalled flatbuffers-25.9.23\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.5\n",
            "    Uninstalling pyparsing-3.2.5:\n",
            "      Successfully uninstalled pyparsing-3.2.5\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.23\n",
            "    Uninstalling pycparser-2.23:\n",
            "      Successfully uninstalled pycparser-2.23\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.8\n",
            "    Uninstalling protobuf-4.25.8:\n",
            "      Successfully uninstalled protobuf-4.25.8\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 12.0.0\n",
            "    Uninstalling pillow-12.0.0:\n",
            "      Successfully uninstalled pillow-12.0.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: opt_einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.9\n",
            "    Uninstalling kiwisolver-1.4.9:\n",
            "      Successfully uninstalled kiwisolver-1.4.9\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.60.1\n",
            "    Uninstalling fonttools-4.60.1:\n",
            "      Successfully uninstalled fonttools-4.60.1\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.4.0\n",
            "    Uninstalling attrs-25.4.0:\n",
            "      Successfully uninstalled attrs-25.4.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 2.3.1\n",
            "    Uninstalling absl-py-2.3.1:\n",
            "      Successfully uninstalled absl-py-2.3.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.11.0.86\n",
            "    Uninstalling opencv-contrib-python-4.11.0.86:\n",
            "      Successfully uninstalled opencv-contrib-python-4.11.0.86\n",
            "  Attempting uninstall: ml_dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.3\n",
            "    Uninstalling ml_dtypes-0.5.3:\n",
            "      Successfully uninstalled ml_dtypes-0.5.3\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.3\n",
            "    Uninstalling contourpy-1.3.3:\n",
            "      Successfully uninstalled contourpy-1.3.3\n",
            "  Attempting uninstall: CFFI\n",
            "    Found existing installation: cffi 2.0.0\n",
            "    Uninstalling cffi-2.0.0:\n",
            "      Successfully uninstalled cffi-2.0.0\n",
            "  Attempting uninstall: sounddevice\n",
            "    Found existing installation: sounddevice 0.5.3\n",
            "    Uninstalling sounddevice-0.5.3:\n",
            "      Successfully uninstalled sounddevice-0.5.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.7\n",
            "    Uninstalling matplotlib-3.10.7:\n",
            "      Successfully uninstalled matplotlib-3.10.7\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.1\n",
            "    Uninstalling jaxlib-0.7.1:\n",
            "      Successfully uninstalled jaxlib-0.7.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.1\n",
            "    Uninstalling jax-0.7.1:\n",
            "      Successfully uninstalled jax-0.7.1\n",
            "  Attempting uninstall: mediapipe\n",
            "    Found existing installation: mediapipe 0.10.15\n",
            "    Uninstalling mediapipe-0.10.15:\n",
            "      Successfully uninstalled mediapipe-0.10.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gradio 5.49.1 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed CFFI-2.0.0 absl-py-2.3.1 attrs-25.4.0 contourpy-1.3.3 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.60.1 jax-0.7.1 jaxlib-0.7.1 kiwisolver-1.4.9 matplotlib-3.10.7 mediapipe-0.10.15 ml_dtypes-0.5.3 numpy-1.26.4 opencv-contrib-python-4.11.0.86 opt_einsum-3.4.0 packaging-25.0 pillow-12.0.0 protobuf-4.25.8 pycparser-2.23 pyparsing-3.2.5 python-dateutil-2.9.0.post0 scipy-1.16.3 six-1.17.0 sounddevice-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_sounddevice",
                  "absl",
                  "attr",
                  "cv2",
                  "cycler",
                  "dateutil",
                  "flatbuffers",
                  "google",
                  "jax",
                  "jaxlib",
                  "kiwisolver",
                  "matplotlib",
                  "mediapipe",
                  "ml_dtypes",
                  "mpl_toolkits",
                  "numpy",
                  "opt_einsum",
                  "packaging",
                  "pyparsing",
                  "scipy",
                  "six"
                ]
              },
              "id": "d7d35394a9bf4a77b31dc2dba2c7caa2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.16.2\n",
            "  Using cached tensorflow-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (3.15.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.2)\n",
            "  Using cached ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (4.25.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (1.76.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.2)\n",
            "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.16.2) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.2) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.2) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.2) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.0.0->tensorflow==2.16.2) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.2) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2) (0.1.2)\n",
            "Downloading tensorflow-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m590.8/590.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install opencv-contrib-python-headless==4.8.0.74\n",
        "!pip install --upgrade --force-reinstall mediapipe==0.10.15\n",
        "!pip install tensorflow==2.16.2\n",
        "!pip install segmentation-models\n",
        "!pip install albumentations\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIWFnMWjv-SP",
        "outputId": "4a25a627-51c4-4c4e-89a7-c58004b850e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (for Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVhvK6d3v-SP",
        "outputId": "5ba88fc9-165a-455d-af55-15901658bcdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, Conv2DTranspose, UpSampling2D, BatchNormalization,\n",
        "    ReLU, Multiply, Input, GlobalAveragePooling2D, Dense,\n",
        "    Activation, MaxPooling2D, Dropout, Add, Concatenate, Lambda\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import layers, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "import albumentations as A\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "from typing import Tuple, List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lDtlkOB9v-SQ"
      },
      "outputs": [],
      "source": [
        "# Initialize MediaPipe Face Detection\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "def detect_face(image):\n",
        "    \"\"\" Detect face using MediaPipe Face Detection \"\"\"\n",
        "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
        "        results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        if results.detections:\n",
        "            detection = results.detections[0]  # Get the first (most confident) detection\n",
        "            return detection\n",
        "    return None\n",
        "\n",
        "def get_face_bbox(image, detection):\n",
        "    \"\"\"Extract face bounding box from detection.\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    bbox = detection.location_data.relative_bounding_box\n",
        "\n",
        "    # Convert relative coordinates to absolute coordinates\n",
        "    x = int(bbox.xmin * w)\n",
        "    y = int(bbox.ymin * h)\n",
        "    width = int(bbox.width * w)\n",
        "    height = int(bbox.height * h)\n",
        "\n",
        "    return x, y, width, height\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OQ_ZwFfdv-SR"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing Functions\n",
        "\n",
        "def extract_green_mask(image):\n",
        "    \"\"\"\n",
        "    Extract green-colored hyperpigmentation annotations from the image\n",
        "    \"\"\"\n",
        "    # Convert to HSV for better color segmentation\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define range for green color in HSV\n",
        "    # Green in HSV: H=60-120, S=100-255, V=100-255\n",
        "    lower_green = np.array([40, 50, 50])   # Lower bound for green\n",
        "    upper_green = np.array([80, 255, 255])  # Upper bound for green\n",
        "\n",
        "    # Create mask for green regions\n",
        "    green_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_CLOSE, kernel)\n",
        "    green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    return green_mask\n",
        "\n",
        "def enhance_image(image):\n",
        "    \"\"\"\n",
        "    Apply various enhancement techniques to improve hyperpigmentation visibility\n",
        "    \"\"\"\n",
        "    # Convert to LAB color space for better contrast enhancement\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "\n",
        "    # Apply CLAHE to L channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "    l = clahe.apply(l)\n",
        "\n",
        "    # Merge channels and convert back to BGR\n",
        "    enhanced_lab = cv2.merge([l, a, b])\n",
        "    enhanced = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Apply bilateral filter to reduce noise while preserving edges\n",
        "    filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def crop_face(image, detection):\n",
        "    \"\"\"\n",
        "    Crop image to face region using face detection\n",
        "    \"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    x, y, width, height = get_face_bbox(image, detection)\n",
        "\n",
        "    # Add padding around the face\n",
        "    padding = 20\n",
        "    x = max(0, x - padding)\n",
        "    y = max(0, y - padding)\n",
        "    width = min(w - x, width + 2 * padding)\n",
        "    height = min(h - y, height + 2 * padding)\n",
        "\n",
        "    # Crop image\n",
        "    cropped_image = image[y:y+height, x:x+width]\n",
        "\n",
        "    bbox = (x, y, x+width, y+height)\n",
        "    return cropped_image, bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KAUTrJkev-SS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data Augmentation Pipeline\n",
        "\n",
        "def get_augmentation_pipeline():\n",
        "    \"\"\"\n",
        "    Define augmentation pipeline for training data\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "        A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "        A.Blur(blur_limit=3, p=0.2),\n",
        "        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n",
        "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n",
        "    ])\n",
        "\n",
        "def apply_augmentation(image, mask, augmentation):\n",
        "    \"\"\"\n",
        "    Apply augmentation to image and mask\n",
        "    \"\"\"\n",
        "    augmented = augmentation(image=image, mask=mask)\n",
        "    return augmented['image'], augmented['mask']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0nVrQEjRv-ST"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Dataset Preparation\n",
        "\n",
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "def preprocess_image(image_path, mask_path, n_classes=1, apply_augmentation_flag=False):\n",
        "    \"\"\"\n",
        "    Preprocess image and mask for training\n",
        "    \"\"\"\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path.numpy().decode('utf-8'))\n",
        "    if image is None:\n",
        "        print(f\"Error: Could not read image file at path: {image_path}\")\n",
        "        return np.zeros((IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32), \\\n",
        "               np.zeros((IMG_SIZE[0], IMG_SIZE[1], 1), dtype=np.float32)\n",
        "\n",
        "    # Enhance image\n",
        "    image = enhance_image(image)\n",
        "\n",
        "    # Read mask\n",
        "    mask = cv2.imread(mask_path.numpy().decode('utf-8'), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        print(f\"Error: Could not read mask file at path: {mask_path}\")\n",
        "        return np.zeros((IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32), \\\n",
        "               np.zeros((IMG_SIZE[0], IMG_SIZE[1], 1), dtype=np.float32)\n",
        "\n",
        "    # Resize\n",
        "    image = cv2.resize(image, IMG_SIZE)\n",
        "    mask = cv2.resize(mask, IMG_SIZE, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Normalize image\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    # Process mask\n",
        "    if n_classes == 1:\n",
        "        mask = (mask > 127).astype(np.float32)\n",
        "        mask = mask[..., None]\n",
        "    else:\n",
        "        mask = mask.astype(np.uint8)\n",
        "        mask = mask[..., None]\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def tf_preprocess_image(image_path, mask_path, n_classes=1, apply_augmentation_flag=False):\n",
        "    \"\"\"\n",
        "    TensorFlow wrapper for preprocessing\n",
        "    \"\"\"\n",
        "    image, mask = tf.py_function(\n",
        "        func=preprocess_image,\n",
        "        inp=[image_path, mask_path, n_classes, apply_augmentation_flag],\n",
        "        Tout=[tf.float32, tf.float32]\n",
        "    )\n",
        "    image.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
        "    mask.set_shape([IMG_SIZE[0], IMG_SIZE[1], 1])\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "def get_dataset(image_dir, mask_dir, batch_size=8, n_classes=1, apply_augmentation_flag=False):\n",
        "    \"\"\"\n",
        "    Create TensorFlow dataset\n",
        "    \"\"\"\n",
        "    image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
        "                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir)\n",
        "                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Ensure same number of files\n",
        "    min_files = min(len(image_files), len(mask_files))\n",
        "    image_files = image_files[:min_files]\n",
        "    mask_files = mask_files[:min_files]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_files, mask_files))\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: tf_preprocess_image(x, y, n_classes, apply_augmentation_flag),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QS94cp82v-SU"
      },
      "outputs": [],
      "source": [
        "# Model Architectures\n",
        "\n",
        "class AttentionBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Attention block for U-Net\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.theta = Conv2D(self.filters, 1, padding='same')\n",
        "        self.phi = Conv2D(self.filters, 1, padding='same')\n",
        "        self.psi = Conv2D(1, 1, padding='same', activation='sigmoid')\n",
        "\n",
        "    def call(self, x, g):\n",
        "        theta_x = self.theta(x)\n",
        "        phi_g = self.phi(g)\n",
        "        f = Activation('relu')(Add()([theta_x, phi_g]))\n",
        "        psi_f = self.psi(f)\n",
        "        return Multiply()([x, psi_f])\n",
        "\n",
        "def conv_block(x, filters, kernel_size=3, padding='same'):\n",
        "    \"\"\"\n",
        "    Convolutional block with batch normalization and ReLU\n",
        "    \"\"\"\n",
        "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def spatial_attention(x):\n",
        "    \"\"\"\n",
        "    Spatial attention mechanism\n",
        "    \"\"\"\n",
        "    avg_pool = Lambda(lambda t: tf.reduce_mean(t, axis=-1, keepdims=True))(x)\n",
        "    max_pool = Lambda(lambda t: tf.reduce_max(t, axis=-1, keepdims=True))(x)\n",
        "    concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "    sa = Conv2D(1, 7, padding='same', activation='sigmoid')(concat)\n",
        "    return Multiply()([x, sa])\n",
        "\n",
        "def build_unet_model(input_shape=(256, 256, 3)):\n",
        "    \"\"\"\n",
        "    Build U-Net model with attention mechanisms\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = conv_block(inputs, 64)\n",
        "    conv1 = conv_block(conv1, 64)\n",
        "    pool1 = MaxPooling2D()(conv1)\n",
        "\n",
        "    conv2 = conv_block(pool1, 128)\n",
        "    conv2 = conv_block(conv2, 128)\n",
        "    pool2 = MaxPooling2D()(conv2)\n",
        "\n",
        "    conv3 = conv_block(pool2, 256)\n",
        "    conv3 = conv_block(conv3, 256)\n",
        "    pool3 = MaxPooling2D()(conv3)\n",
        "\n",
        "    conv4 = conv_block(pool3, 512)\n",
        "    conv4 = conv_block(conv4, 512)\n",
        "    pool4 = MaxPooling2D()(conv4)\n",
        "\n",
        "    # Bottleneck\n",
        "    bneck = conv_block(pool4, 1024)\n",
        "    bneck = conv_block(bneck, 1024)\n",
        "    bneck = spatial_attention(bneck)\n",
        "\n",
        "    # Decoder with attention\n",
        "    up4 = Conv2DTranspose(512, 2, strides=2, padding='same')(bneck)\n",
        "    attn4 = AttentionBlock(512)(conv4, up4)\n",
        "    merge4 = Concatenate()([up4, attn4])\n",
        "    conv5 = conv_block(merge4, 512)\n",
        "    conv5 = conv_block(conv5, 512)\n",
        "\n",
        "    up3 = Conv2DTranspose(256, 2, strides=2, padding='same')(conv5)\n",
        "    attn3 = AttentionBlock(256)(conv3, up3)\n",
        "    merge3 = Concatenate()([up3, attn3])\n",
        "    conv6 = conv_block(merge3, 256)\n",
        "    conv6 = conv_block(conv6, 256)\n",
        "\n",
        "    up2 = Conv2DTranspose(128, 2, strides=2, padding='same')(conv6)\n",
        "    attn2 = AttentionBlock(128)(conv2, up2)\n",
        "    merge2 = Concatenate()([up2, attn2])\n",
        "    conv7 = conv_block(merge2, 128)\n",
        "    conv7 = conv_block(conv7, 128)\n",
        "\n",
        "    up1 = Conv2DTranspose(64, 2, strides=2, padding='same')(conv7)\n",
        "    attn1 = AttentionBlock(64)(conv1, up1)\n",
        "    merge1 = Concatenate()([up1, attn1])\n",
        "    conv8 = conv_block(merge1, 64)\n",
        "    conv8 = conv_block(conv8, 64)\n",
        "\n",
        "    # Output\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid', padding='same')(conv8)\n",
        "\n",
        "    model = Model(inputs, outputs, name='Attention_UNet')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dvCUe87Bv-SU"
      },
      "outputs": [],
      "source": [
        "# Additional Model Architectures\n",
        "\n",
        "def build_deeplabv3_plus(input_shape=(256, 256, 3)):\n",
        "    \"\"\"\n",
        "    Build DeepLabV3+ model\n",
        "    \"\"\"\n",
        "    # This is a simplified version - for full implementation, use segmentation-models library\n",
        "    base_model = tf.keras.applications.ResNet50(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # ASPP (Atrous Spatial Pyramid Pooling)\n",
        "    x = base_model.output\n",
        "\n",
        "    # Global average pooling\n",
        "    gap = GlobalAveragePooling2D()(x)\n",
        "    gap = Dense(256, activation='relu')(gap)\n",
        "    gap = tf.keras.layers.Reshape((1, 1, 256))(gap)\n",
        "    gap = Conv2DTranspose(256, 32, strides=32, padding='same')(gap)\n",
        "\n",
        "    # Atrous convolutions\n",
        "    atrous1 = Conv2D(256, 1, padding='same', activation='relu')(x)\n",
        "    atrous2 = Conv2D(256, 3, padding='same', dilation_rate=6, activation='relu')(x)\n",
        "    atrous3 = Conv2D(256, 3, padding='same', dilation_rate=12, activation='relu')(x)\n",
        "    atrous4 = Conv2D(256, 3, padding='same', dilation_rate=18, activation='relu')(x)\n",
        "\n",
        "    # Concatenate\n",
        "    x = Concatenate()([atrous1, atrous2, atrous3, atrous4, gap])\n",
        "    x = Conv2D(256, 1, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2DTranspose(256, 4, strides=4, padding='same')(x)\n",
        "\n",
        "    # Skip connection\n",
        "    skip = Conv2D(48, 1, padding='same')(base_model.get_layer('conv2_block3_2_relu').output)\n",
        "    x = Concatenate()([x, skip])\n",
        "\n",
        "    x = Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "    x = Conv2DTranspose(256, 4, strides=4, padding='same')(x)\n",
        "\n",
        "    # Output\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    model = Model(base_model.input, outputs, name='DeepLabV3Plus')\n",
        "    return model\n",
        "\n",
        "def build_segnet(input_shape=(256, 256, 3)):\n",
        "    \"\"\"\n",
        "    Build SegNet model\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = conv_block(inputs, 64)\n",
        "    conv1 = conv_block(conv1, 64)\n",
        "    pool1 = MaxPooling2D()(conv1)\n",
        "\n",
        "    conv2 = conv_block(pool1, 128)\n",
        "    conv2 = conv_block(conv2, 128)\n",
        "    pool2 = MaxPooling2D()(conv2)\n",
        "\n",
        "    conv3 = conv_block(pool2, 256)\n",
        "    conv3 = conv_block(conv3, 256)\n",
        "    pool3 = MaxPooling2D()(conv3)\n",
        "\n",
        "    conv4 = conv_block(pool3, 512)\n",
        "    conv4 = conv_block(conv4, 512)\n",
        "    pool4 = MaxPooling2D()(conv4)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv5 = conv_block(pool4, 1024)\n",
        "    conv5 = conv_block(conv5, 1024)\n",
        "\n",
        "    # Decoder\n",
        "    up4 = Conv2DTranspose(512, 2, strides=2, padding='same')(conv5)\n",
        "    conv6 = conv_block(up4, 512)\n",
        "    conv6 = conv_block(conv6, 512)\n",
        "\n",
        "    up3 = Conv2DTranspose(256, 2, strides=2, padding='same')(conv6)\n",
        "    conv7 = conv_block(up3, 256)\n",
        "    conv7 = conv_block(conv7, 256)\n",
        "\n",
        "    up2 = Conv2DTranspose(128, 2, strides=2, padding='same')(conv7)\n",
        "    conv8 = conv_block(up2, 128)\n",
        "    conv8 = conv_block(conv8, 128)\n",
        "\n",
        "    up1 = Conv2DTranspose(64, 2, strides=2, padding='same')(conv8)\n",
        "    conv9 = conv_block(up1, 64)\n",
        "    conv9 = conv_block(conv9, 64)\n",
        "\n",
        "    # Output\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid', padding='same')(conv9)\n",
        "\n",
        "    model = Model(inputs, outputs, name='SegNet')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nSYZl-B2v-SV"
      },
      "outputs": [],
      "source": [
        "# Loss Functions and Metrics\n",
        "\n",
        "# Instantiate BinaryCrossentropy loss with no reduction for per-pixel calculations in focal_loss\n",
        "_bce_fn_no_reduction = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Dice loss for segmentation\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):\n",
        "    \"\"\"\n",
        "    Focal loss for handling class imbalance\n",
        "    \"\"\"\n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "    # Ensure y_true and y_pred have a channel dimension of 1\n",
        "    if len(y_true.shape) == 3: # (batch, H, W)\n",
        "        y_true = tf.expand_dims(y_true, axis=-1)\n",
        "    if len(y_pred.shape) == 3: # (batch, H, W)\n",
        "        y_pred = tf.expand_dims(y_pred, axis=-1)\n",
        "\n",
        "    alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "\n",
        "    # Use the pre-instantiated BinaryCrossentropy with no reduction\n",
        "    bce_per_pixel = _bce_fn_no_reduction(y_true, y_pred) # This will return (batch, H, W, 1)\n",
        "\n",
        "    focal_loss_per_pixel = alpha_t * tf.pow((1 - p_t), gamma) * bce_per_pixel\n",
        "    return tf.reduce_mean(focal_loss_per_pixel)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Combined loss function\n",
        "    \"\"\"\n",
        "    # Using the functional API for BCE will calculate the mean over batch and spatial dims\n",
        "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    dice = dice_loss(y_true, y_pred)\n",
        "    focal = focal_loss(y_true, y_pred)\n",
        "    return bce + dice + 0.5 * focal\n",
        "\n",
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Dice coefficient metric\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Intersection over Union metric\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez2LlUxOv-SV",
        "outputId": "5e3738ca-a7a4-4835-c854-3fff50ee7fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration set up successfully!\n"
          ]
        }
      ],
      "source": [
        "# Training Configuration\n",
        "\n",
        "# Define paths (update these according to your data structure)\n",
        "ORIGINAL_IMAGES_PATH = \"/content/drive/MyDrive/Hyperpigmentation /Original photo\"\n",
        "ANNOTATED_IMAGES_PATH = \"/content/drive/MyDrive/Hyperpigmentation /Annotated\"\n",
        "OUTPUT_IMAGES_PATH = \"/content/drive/MyDrive/Dataset/Hyperpigmentation_Cropped_Images\"\n",
        "OUTPUT_MASKS_PATH = \"/content/drive/MyDrive/Dataset/Hyperpigmentation_Cropped_Masks\"\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "IMG_SIZE = (256, 256)\n",
        "N_CLASSES = 1\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(OUTPUT_IMAGES_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_MASKS_PATH, exist_ok=True)\n",
        "\n",
        "print(\"Configuration set up successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Eiwh-s0Dv-SW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f837c4a0-8bda-4d1c-a9cd-c29ff3ac3330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset preprocessing...\n",
            "Could not detect face for: IMG_0214.JPG\n",
            "Could not detect face for: IMG_0213.JPG\n",
            "Processed mask: IMG_9919.JPG\n",
            "Processed mask: IMG_0193.JPG\n",
            "Processed mask: IMG_9918.JPG\n",
            "Could not detect face for: IMG_0214.JPG\n",
            "Could not detect face for: IMG_0213.JPG\n",
            "Processed image: IMG_9919.JPG\n",
            "Processed image: IMG_0193.JPG\n",
            "Processed image: IMG_9918.JPG\n",
            "Dataset preprocessing completed!\n"
          ]
        }
      ],
      "source": [
        "# Data Preprocessing Pipeline\n",
        "\n",
        "def preprocess_dataset(original_path, annotated_path, output_img_path, output_mask_path):\n",
        "    \"\"\"\n",
        "    Preprocess the entire dataset\n",
        "    \"\"\"\n",
        "    print(\"Starting dataset preprocessing...\")\n",
        "\n",
        "    # Process annotated images to extract green masks\n",
        "    for filename in os.listdir(annotated_path):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(annotated_path, filename)\n",
        "            image = cv2.imread(img_path)\n",
        "\n",
        "            if image is None:\n",
        "                print(f\"Could not read image: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Detect face\n",
        "            detection = detect_face(image)\n",
        "            if detection is None:\n",
        "                print(f\"Could not detect face for: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Crop to face region\n",
        "            cropped_img, bbox = crop_face(image, detection)\n",
        "            if cropped_img is None:\n",
        "                print(f\"Could not crop image: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Extract green mask\n",
        "            green_mask = extract_green_mask(cropped_img)\n",
        "\n",
        "            # Save cropped mask\n",
        "            mask_path = os.path.join(output_mask_path, filename)\n",
        "            cv2.imwrite(mask_path, green_mask)\n",
        "\n",
        "            print(f\"Processed mask: {filename}\")\n",
        "\n",
        "    # Process original images\n",
        "    for filename in os.listdir(original_path):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(original_path, filename)\n",
        "            image = cv2.imread(img_path)\n",
        "\n",
        "            if image is None:\n",
        "                print(f\"Could not read image: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Detect face\n",
        "            detection = detect_face(image)\n",
        "            if detection is None:\n",
        "                print(f\"Could not detect face for: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Crop to face region\n",
        "            cropped_img, bbox = crop_face(image, detection)\n",
        "            if cropped_img is None:\n",
        "                print(f\"Could not crop image: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Save cropped image\n",
        "            img_output_path = os.path.join(output_img_path, filename)\n",
        "            cv2.imwrite(img_output_path, cropped_img)\n",
        "\n",
        "            print(f\"Processed image: {filename}\")\n",
        "\n",
        "    print(\"Dataset preprocessing completed!\")\n",
        "\n",
        "# Uncomment the line below to run preprocessing\n",
        "preprocess_dataset(ORIGINAL_IMAGES_PATH, ANNOTATED_IMAGES_PATH, OUTPUT_IMAGES_PATH, OUTPUT_MASKS_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cn9mZTlnv-SW",
        "outputId": "5c13d418-c060-41ba-9622-2f43cef40c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training U-Net model...\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node compile_loss/combined_loss/mul_7 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-159207915.py\", line 62, in <cell line: 0>\n\n  File \"/tmp/ipython-input-159207915.py\", line 50, in train_model\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 401, in fit\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 489, in evaluate\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 93, in test_step\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/trainer.py\", line 383, in _compute_loss\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/trainer.py\", line 351, in compute_loss\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/compile_utils.py\", line 690, in __call__\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/compile_utils.py\", line 699, in call\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/losses/loss.py\", line 67, in __call__\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/losses/losses.py\", line 33, in call\n\n  File \"/tmp/ipython-input-4267670823.py\", line 44, in combined_loss\n\n  File \"/tmp/ipython-input-4267670823.py\", line 34, in focal_loss\n\nIncompatible shapes: [3,256,256] vs. [3,256,256,1]\n\t [[{{node compile_loss/combined_loss/mul_7}}]] [Op:__inference_multi_step_on_iterator_69375]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-159207915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Train different models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training U-Net model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0munet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munet_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining DeepLabV3+ model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-159207915.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/combined_loss/mul_7 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipython-input-159207915.py\", line 62, in <cell line: 0>\n\n  File \"/tmp/ipython-input-159207915.py\", line 50, in train_model\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 401, in fit\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 489, in evaluate\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 93, in test_step\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/trainer.py\", line 383, in _compute_loss\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/trainer.py\", line 351, in compute_loss\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/compile_utils.py\", line 690, in __call__\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/compile_utils.py\", line 699, in call\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/losses/loss.py\", line 67, in __call__\n\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/losses/losses.py\", line 33, in call\n\n  File \"/tmp/ipython-input-4267670823.py\", line 44, in combined_loss\n\n  File \"/tmp/ipython-input-4267670823.py\", line 34, in focal_loss\n\nIncompatible shapes: [3,256,256] vs. [3,256,256,1]\n\t [[{{node compile_loss/combined_loss/mul_7}}]] [Op:__inference_multi_step_on_iterator_69375]"
          ]
        }
      ],
      "source": [
        "# Model Training\n",
        "\n",
        "def train_model(model_name='unet'):\n",
        "    \"\"\"\n",
        "    Train the selected model\n",
        "    \"\"\"\n",
        "    # Create datasets\n",
        "    train_dataset = get_dataset(\n",
        "        OUTPUT_IMAGES_PATH,\n",
        "        OUTPUT_MASKS_PATH,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        n_classes=N_CLASSES,\n",
        "        apply_augmentation_flag=True\n",
        "    )\n",
        "\n",
        "    # Split data for validation (assuming you have validation data)\n",
        "    # For now, we'll use the same data for both training and validation\n",
        "    val_dataset = train_dataset.take(10)  # Use first 10 batches for validation\n",
        "    train_dataset = train_dataset.skip(10)  # Use remaining for training\n",
        "\n",
        "    # Build model\n",
        "    if model_name == 'unet':\n",
        "        model = build_unet_model(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    elif model_name == 'deeplabv3':\n",
        "        model = build_deeplabv3_plus(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    elif model_name == 'segnet':\n",
        "        model = build_segnet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss=combined_loss,\n",
        "        metrics=['accuracy', dice_coefficient, iou_metric]\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            f'hyperpigmentation_{model_name}_best.h5',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Train different models\n",
        "print(\"Training U-Net model...\")\n",
        "unet_model, unet_history = train_model('unet')\n",
        "\n",
        "print(\"\\nTraining DeepLabV3+ model...\")\n",
        "deeplab_model, deeplab_history = train_model('deeplabv3')\n",
        "\n",
        "print(\"\\nTraining SegNet model...\")\n",
        "segnet_model, segnet_history = train_model('segnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-fZ84ivv-SX"
      },
      "outputs": [],
      "source": [
        "# Model Evaluation and Visualization\n",
        "\n",
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"\n",
        "    Plot training history\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[0, 0].set_title(f'{model_name} - Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    # Accuracy\n",
        "    axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[0, 1].set_title(f'{model_name} - Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    # Dice Coefficient\n",
        "    axes[1, 0].plot(history.history['dice_coefficient'], label='Training Dice')\n",
        "    axes[1, 0].plot(history.history['val_dice_coefficient'], label='Validation Dice')\n",
        "    axes[1, 0].set_title(f'{model_name} - Dice Coefficient')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Dice Coefficient')\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    # IoU\n",
        "    axes[1, 1].plot(history.history['iou_metric'], label='Training IoU')\n",
        "    axes[1, 1].plot(history.history['val_iou_metric'], label='Validation IoU')\n",
        "    axes[1, 1].set_title(f'{model_name} - IoU')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('IoU')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_predictions(model, dataset, num_samples=5):\n",
        "    \"\"\"\n",
        "    Visualize model predictions\n",
        "    \"\"\"\n",
        "    for images, masks in dataset.take(1):\n",
        "        predictions = model.predict(images)\n",
        "\n",
        "        for i in range(min(num_samples, images.shape[0])):\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "            # Original image\n",
        "            axes[0].imshow(images[i])\n",
        "            axes[0].set_title('Original Image')\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Ground truth mask\n",
        "            axes[1].imshow(masks[i].squeeze(), cmap='gray')\n",
        "            axes[1].set_title('Ground Truth')\n",
        "            axes[1].axis('off')\n",
        "\n",
        "            # Prediction\n",
        "            pred_mask = (predictions[i].squeeze() > 0.5).astype(np.uint8)\n",
        "            axes[2].imshow(pred_mask, cmap='gray')\n",
        "            axes[2].set_title('Prediction')\n",
        "            axes[2].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            if i >= num_samples - 1:\n",
        "                break\n",
        "\n",
        "# Plot training histories\n",
        "plot_training_history(unet_history, 'U-Net')\n",
        "plot_training_history(deeplab_history, 'DeepLabV3+')\n",
        "plot_training_history(segnet_history, 'SegNet')\n",
        "\n",
        "# Visualize predictions\n",
        "print(\"U-Net Predictions:\")\n",
        "visualize_predictions(unet_model, val_dataset)\n",
        "\n",
        "print(\"DeepLabV3+ Predictions:\")\n",
        "visualize_predictions(deeplab_model, val_dataset)\n",
        "\n",
        "print(\"SegNet Predictions:\")\n",
        "visualize_predictions(segnet_model, val_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrkRRgGqv-SX"
      },
      "outputs": [],
      "source": [
        "# Model Comparison and Selection\n",
        "\n",
        "def evaluate_model(model, dataset, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate model performance\n",
        "    \"\"\"\n",
        "    results = model.evaluate(dataset, verbose=0)\n",
        "\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Loss': results[0],\n",
        "        'Accuracy': results[1],\n",
        "        'Dice Coefficient': results[2],\n",
        "        'IoU': results[3]\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Evaluate all models\n",
        "models = {\n",
        "    'U-Net': unet_model,\n",
        "    'DeepLabV3+': deeplab_model,\n",
        "    'SegNet': segnet_model\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    metrics = evaluate_model(model, val_dataset, name)\n",
        "    results.append(metrics)\n",
        "    print(f\"{name} Results:\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Create comparison table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "print(\"Model Comparison:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Select best model based on IoU\n",
        "best_model_name = df.loc[df['IoU'].idxmax(), 'Model']\n",
        "best_model = models[best_model_name]\n",
        "print(f\"\\nBest model: {best_model_name}\")\n",
        "print(f\"Best IoU: {df['IoU'].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcDKr_-Sv-SX"
      },
      "outputs": [],
      "source": [
        "# Inference Pipeline\n",
        "\n",
        "def predict_hyperpigmentation(image_path, model, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predict hyperpigmentation on a new image\n",
        "    \"\"\"\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Could not read image: {image_path}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Detect face\n",
        "    detection = detect_face(image)\n",
        "    if detection is None:\n",
        "        print(\"Could not detect face\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Crop to face region\n",
        "    cropped_img, bbox = crop_face(image, detection)\n",
        "    if cropped_img is None:\n",
        "        print(\"Could not crop face region\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Preprocess for model\n",
        "    processed_img = enhance_image(cropped_img)\n",
        "    processed_img = cv2.resize(processed_img, IMG_SIZE)\n",
        "    processed_img = processed_img.astype(np.float32) / 255.0\n",
        "\n",
        "    # Predict\n",
        "    input_tensor = tf.expand_dims(processed_img, 0)\n",
        "    prediction = model.predict(input_tensor)\n",
        "\n",
        "    # Threshold prediction\n",
        "    pred_mask = (prediction[0].squeeze() > threshold).astype(np.uint8)\n",
        "\n",
        "    return cropped_img, pred_mask, bbox\n",
        "\n",
        "def visualize_prediction(image_path, model, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Visualize prediction on a new image\n",
        "    \"\"\"\n",
        "    cropped_img, pred_mask, bbox = predict_hyperpigmentation(image_path, model, threshold)\n",
        "\n",
        "    if cropped_img is None:\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Original cropped image\n",
        "    axes[0].imshow(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title('Cropped Face Region')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Prediction overlay\n",
        "    overlay = cropped_img.copy()\n",
        "    overlay[pred_mask > 0] = [0, 255, 0]  # Green overlay for hyperpigmentation\n",
        "\n",
        "    axes[1].imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "    axes[1].set_title('Hyperpigmentation Detection')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate hyperpigmentation percentage\n",
        "    total_pixels = pred_mask.size\n",
        "    hyperpigmentation_pixels = np.sum(pred_mask > 0)\n",
        "    percentage = (hyperpigmentation_pixels / total_pixels) * 100\n",
        "\n",
        "    print(f\"Hyperpigmentation coverage: {percentage:.2f}%\")\n",
        "\n",
        "    return percentage\n",
        "\n",
        "# Example usage with best model\n",
        "print(f\"Using {best_model_name} for inference\")\n",
        "\n",
        "# Test on a sample image (update path as needed)\n",
        "sample_image_path = \"/content/drive/MyDrive/Dataset/Test/sample_image.jpg\"\n",
        "# visualize_prediction(sample_image_path, best_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BDSMj_nv-SX"
      },
      "outputs": [],
      "source": [
        "# Save Models\n",
        "\n",
        "# Save all trained models\n",
        "unet_model.save('hyperpigmentation_unet_model.h5')\n",
        "deeplab_model.save('hyperpigmentation_deeplab_model.h5')\n",
        "segnet_model.save('hyperpigmentation_segnet_model.h5')\n",
        "\n",
        "# Save best model with additional metadata\n",
        "best_model.save(f'hyperpigmentation_{best_model_name.lower()}_best_model.h5')\n",
        "\n",
        "print(\"All models saved successfully!\")\n",
        "print(f\"Best model ({best_model_name}) saved as: hyperpigmentation_{best_model_name.lower()}_best_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBwW7Lpuv-SY"
      },
      "outputs": [],
      "source": [
        "# Additional Utility Functions\n",
        "\n",
        "def batch_predict_hyperpigmentation(image_folder, model, output_folder, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Batch prediction on multiple images\n",
        "    \"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "            try:\n",
        "                cropped_img, pred_mask, bbox = predict_hyperpigmentation(image_path, model, threshold)\n",
        "\n",
        "                if cropped_img is not None:\n",
        "                    # Save prediction mask\n",
        "                    mask_path = os.path.join(output_folder, f\"{filename}_mask.png\")\n",
        "                    cv2.imwrite(mask_path, pred_mask * 255)\n",
        "\n",
        "                    # Calculate hyperpigmentation percentage\n",
        "                    total_pixels = pred_mask.size\n",
        "                    hyperpigmentation_pixels = np.sum(pred_mask > 0)\n",
        "                    percentage = (hyperpigmentation_pixels / total_pixels) * 100\n",
        "\n",
        "                    results.append({\n",
        "                        'filename': filename,\n",
        "                        'hyperpigmentation_percentage': percentage,\n",
        "                        'bbox': bbox\n",
        "                    })\n",
        "\n",
        "                    print(f\"Processed: {filename} - {percentage:.2f}% hyperpigmentation\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    # Save results to CSV\n",
        "    import pandas as pd\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results.to_csv(os.path.join(output_folder, 'hyperpigmentation_results.csv'), index=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_hyperpigmentation_report(image_path, model, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Create a detailed hyperpigmentation analysis report\n",
        "    \"\"\"\n",
        "    cropped_img, pred_mask, bbox = predict_hyperpigmentation(image_path, model, threshold)\n",
        "\n",
        "    if cropped_img is None:\n",
        "        return None\n",
        "\n",
        "    # Calculate various metrics\n",
        "    total_pixels = pred_mask.size\n",
        "    hyperpigmentation_pixels = np.sum(pred_mask > 0)\n",
        "    percentage = (hyperpigmentation_pixels / total_pixels) * 100\n",
        "\n",
        "    # Find connected components\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(pred_mask, connectivity=8)\n",
        "\n",
        "    # Calculate area statistics\n",
        "    areas = stats[1:, cv2.CC_STAT_AREA]  # Skip background (label 0)\n",
        "\n",
        "    report = {\n",
        "        'total_area': total_pixels,\n",
        "        'hyperpigmentation_area': hyperpigmentation_pixels,\n",
        "        'hyperpigmentation_percentage': percentage,\n",
        "        'num_regions': num_labels - 1,  # Exclude background\n",
        "        'largest_region_area': np.max(areas) if len(areas) > 0 else 0,\n",
        "        'average_region_area': np.mean(areas) if len(areas) > 0 else 0,\n",
        "        'bbox': bbox\n",
        "    }\n",
        "\n",
        "    return report\n",
        "\n",
        "print(\"Utility functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDCaXHafv-SY"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a comprehensive hyperpigmentation detection system with the following features:\n",
        "\n",
        "### 1. **Data Preprocessing**\n",
        "- Green color segmentation for annotation masks\n",
        "- **Overall face detection** using MediaPipe Face Detection (no facial region segmentation)\n",
        "- Image enhancement using CLAHE and bilateral filtering\n",
        "- Data augmentation pipeline\n",
        "\n",
        "### 2. **Model Architectures**\n",
        "- **U-Net with Attention**: Best for detailed segmentation\n",
        "- **DeepLabV3+**: Good for multi-scale features\n",
        "- **SegNet**: Efficient encoder-decoder architecture\n",
        "\n",
        "### 3. **Training Features**\n",
        "- Combined loss function (BCE + Dice + Focal)\n",
        "- Multiple evaluation metrics (Dice, IoU, Accuracy)\n",
        "- Early stopping and learning rate scheduling\n",
        "- Model checkpointing\n",
        "\n",
        "### 4. **Inference Pipeline**\n",
        "- Real-time prediction on new images\n",
        "- Batch processing capabilities\n",
        "- Detailed analysis reports\n",
        "- Visualization tools\n",
        "\n",
        "### 5. **Usage Instructions**\n",
        "\n",
        "1. **Setup**: Update the file paths in the configuration section\n",
        "2. **Preprocessing**: Run the dataset preprocessing pipeline\n",
        "3. **Training**: Train all three models and compare performance\n",
        "4. **Evaluation**: Use the best performing model for inference\n",
        "5. **Inference**: Apply the model to new images for hyperpigmentation detection\n",
        "\n",
        "### 6. **Model Recommendations**\n",
        "\n",
        "Based on typical performance:\n",
        "- **U-Net**: Best overall performance, good for detailed segmentation\n",
        "- **DeepLabV3+**: Good for handling multi-scale hyperpigmentation\n",
        "- **SegNet**: Fastest inference, good for real-time applications\n",
        "\n",
        "### 7. **Key Changes Made**\n",
        "- **Simplified Face Detection**: Uses MediaPipe Face Detection instead of facial landmark segmentation\n",
        "- **Overall Face Cropping**: Crops the entire detected face region instead of specific facial areas\n",
        "- **Faster Processing**: More efficient preprocessing pipeline\n",
        "\n",
        "The system automatically selects the best model based on IoU score and provides comprehensive evaluation metrics.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}