{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperpigmentation Detection System\n",
        "\n",
        "This notebook implements a comprehensive hyperpigmentation detection system using deep learning models. The system processes original facial images and green-annotated hyperpigmentation masks to train various segmentation models.\n",
        "\n",
        "## Features:\n",
        "- Multiple preprocessing techniques for facial images\n",
        "- Green color segmentation for annotation masks\n",
        "- Multiple model architectures (U-Net, DeepLabV3+, SegNet)\n",
        "- Comprehensive evaluation metrics\n",
        "- Real-time inference pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install opencv-contrib-python-headless==4.8.0.74\n",
        "!pip install --upgrade --force-reinstall mediapipe==0.10.5\n",
        "!pip install tensorflow==2.13.0\n",
        "!pip install segmentation-models\n",
        "!pip install albumentations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (for Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, Conv2DTranspose, UpSampling2D, BatchNormalization, \n",
        "    ReLU, Multiply, Input, GlobalAveragePooling2D, Dense, \n",
        "    Activation, MaxPooling2D, Dropout, Add, Concatenate\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import layers, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "import albumentations as A\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "from typing import Tuple, List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize MediaPipe Face Detection\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "def detect_face(image):\n",
        "    \"\"\" Detect face using MediaPipe Face Detection \"\"\"\n",
        "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
        "        results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        if results.detections:\n",
        "            detection = results.detections[0]  # Get the first (most confident) detection\n",
        "            return detection\n",
        "    return None\n",
        "\n",
        "def get_face_bbox(image, detection):\n",
        "    \"\"\"Extract face bounding box from detection.\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    bbox = detection.location_data.relative_bounding_box\n",
        "    \n",
        "    # Convert relative coordinates to absolute coordinates\n",
        "    x = int(bbox.xmin * w)\n",
        "    y = int(bbox.ymin * h)\n",
        "    width = int(bbox.width * w)\n",
        "    height = int(bbox.height * h)\n",
        "    \n",
        "    return x, y, width, height\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Preprocessing Functions\n",
        "\n",
        "def extract_green_mask(image):\n",
        "    \"\"\"\n",
        "    Extract green-colored hyperpigmentation annotations from the image\n",
        "    \"\"\"\n",
        "    # Convert to HSV for better color segmentation\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    \n",
        "    # Define range for green color in HSV\n",
        "    # Green in HSV: H=60-120, S=100-255, V=100-255\n",
        "    lower_green = np.array([40, 50, 50])   # Lower bound for green\n",
        "    upper_green = np.array([80, 255, 255])  # Upper bound for green\n",
        "    \n",
        "    # Create mask for green regions\n",
        "    green_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
        "    \n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_CLOSE, kernel)\n",
        "    green_mask = cv2.morphologyEx(green_mask, cv2.MORPH_OPEN, kernel)\n",
        "    \n",
        "    return green_mask\n",
        "\n",
        "def enhance_image(image):\n",
        "    \"\"\"\n",
        "    Apply various enhancement techniques to improve hyperpigmentation visibility\n",
        "    \"\"\"\n",
        "    # Convert to LAB color space for better contrast enhancement\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    \n",
        "    # Apply CLAHE to L channel\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "    l = clahe.apply(l)\n",
        "    \n",
        "    # Merge channels and convert back to BGR\n",
        "    enhanced_lab = cv2.merge([l, a, b])\n",
        "    enhanced = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
        "    \n",
        "    # Apply bilateral filter to reduce noise while preserving edges\n",
        "    filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "def crop_face(image, detection):\n",
        "    \"\"\"\n",
        "    Crop image to face region using face detection\n",
        "    \"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    x, y, width, height = get_face_bbox(image, detection)\n",
        "    \n",
        "    # Add padding around the face\n",
        "    padding = 20\n",
        "    x = max(0, x - padding)\n",
        "    y = max(0, y - padding)\n",
        "    width = min(w - x, width + 2 * padding)\n",
        "    height = min(h - y, height + 2 * padding)\n",
        "    \n",
        "    # Crop image\n",
        "    cropped_image = image[y:y+height, x:x+width]\n",
        "    \n",
        "    bbox = (x, y, x+width, y+height)\n",
        "    return cropped_image, bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Augmentation Pipeline\n",
        "\n",
        "def get_augmentation_pipeline():\n",
        "    \"\"\"\n",
        "    Define augmentation pipeline for training data\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "        A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "        A.Blur(blur_limit=3, p=0.2),\n",
        "        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),\n",
        "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n",
        "    ])\n",
        "\n",
        "def apply_augmentation(image, mask, augmentation):\n",
        "    \"\"\"\n",
        "    Apply augmentation to image and mask\n",
        "    \"\"\"\n",
        "    augmented = augmentation(image=image, mask=mask)\n",
        "    return augmented['image'], augmented['mask']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "def preprocess_image(image_path, mask_path, n_classes=1, apply_augmentation_flag=False):\n",
        "    \"\"\"\n",
        "    Preprocess image and mask for training\n",
        "    \"\"\"\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path.numpy().decode('utf-8'))\n",
        "    if image is None:\n",
        "        print(f\"Error: Could not read image file at path: {image_path}\")\n",
        "        return np.zeros((IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32), \\\n",
        "               np.zeros((IMG_SIZE[0], IMG_SIZE[1], 1), dtype=np.float32)\n",
        "    \n",
        "    # Enhance image\n",
        "    image = enhance_image(image)\n",
        "    \n",
        "    # Read mask\n",
        "    mask = cv2.imread(mask_path.numpy().decode('utf-8'), cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        print(f\"Error: Could not read mask file at path: {mask_path}\")\n",
        "        return np.zeros((IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32), \\\n",
        "               np.zeros((IMG_SIZE[0], IMG_SIZE[1], 1), dtype=np.float32)\n",
        "    \n",
        "    # Resize\n",
        "    image = cv2.resize(image, IMG_SIZE)\n",
        "    mask = cv2.resize(mask, IMG_SIZE, interpolation=cv2.INTER_NEAREST)\n",
        "    \n",
        "    # Normalize image\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    \n",
        "    # Process mask\n",
        "    if n_classes == 1:\n",
        "        mask = (mask > 127).astype(np.float32)\n",
        "        mask = mask[..., None]\n",
        "    else:\n",
        "        mask = mask.astype(np.uint8)\n",
        "        mask = mask[..., None]\n",
        "    \n",
        "    return image, mask\n",
        "\n",
        "def tf_preprocess_image(image_path, mask_path, n_classes=1, apply_augmentation_flag=False):\n",
        "    \"\"\"\n",
        "    TensorFlow wrapper for preprocessing\n",
        "    \"\"\"\n",
        "    image, mask = tf.py_function(\n",
        "        func=preprocess_image,\n",
        "        inp=[image_path, mask_path, n_classes, apply_augmentation_flag],\n",
        "        Tout=[tf.float32, tf.float32]\n",
        "    )\n",
        "    image.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
        "    mask.set_shape([IMG_SIZE[0], IMG_SIZE[1], 1])\n",
        "    \n",
        "    return image, mask\n",
        "\n",
        "def get_dataset(image_dir, mask_dir, batch_size=8, n_classes=1, apply_augmentation_flag=False):\n",
        "    \"\"\"\n",
        "    Create TensorFlow dataset\n",
        "    \"\"\"\n",
        "    image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) \n",
        "                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) \n",
        "                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    \n",
        "    # Ensure same number of files\n",
        "    min_files = min(len(image_files), len(mask_files))\n",
        "    image_files = image_files[:min_files]\n",
        "    mask_files = mask_files[:min_files]\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_files, mask_files))\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: tf_preprocess_image(x, y, n_classes, apply_augmentation_flag), \n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Architectures\n",
        "\n",
        "class AttentionBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Attention block for U-Net\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.theta = Conv2D(self.filters, 1, padding='same')\n",
        "        self.phi = Conv2D(self.filters, 1, padding='same')\n",
        "        self.psi = Conv2D(1, 1, padding='same', activation='sigmoid')\n",
        "        \n",
        "    def call(self, x, g):\n",
        "        theta_x = self.theta(x)\n",
        "        phi_g = self.phi(g)\n",
        "        f = Activation('relu')(Add()([theta_x, phi_g]))\n",
        "        psi_f = self.psi(f)\n",
        "        return Multiply()([x, psi_f])\n",
        "\n",
        "def conv_block(x, filters, kernel_size=3, padding='same'):\n",
        "    \"\"\"\n",
        "    Convolutional block with batch normalization and ReLU\n",
        "    \"\"\"\n",
        "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def spatial_attention(x):\n",
        "    \"\"\"\n",
        "    Spatial attention mechanism\n",
        "    \"\"\"\n",
        "    avg_pool = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "    max_pool = tf.reduce_max(x, axis=-1, keepdims=True)\n",
        "    concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "    sa = Conv2D(1, 7, padding='same', activation='sigmoid')(concat)\n",
        "    return Multiply()([x, sa])\n",
        "\n",
        "def build_unet_model(input_shape=(256, 256, 3)):\n",
        "    \"\"\"\n",
        "    Build U-Net model with attention mechanisms\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    # Encoder\n",
        "    conv1 = conv_block(inputs, 64)\n",
        "    conv1 = conv_block(conv1, 64)\n",
        "    pool1 = MaxPooling2D()(conv1)\n",
        "    \n",
        "    conv2 = conv_block(pool1, 128)\n",
        "    conv2 = conv_block(conv2, 128)\n",
        "    pool2 = MaxPooling2D()(conv2)\n",
        "    \n",
        "    conv3 = conv_block(pool2, 256)\n",
        "    conv3 = conv_block(conv3, 256)\n",
        "    pool3 = MaxPooling2D()(conv3)\n",
        "    \n",
        "    conv4 = conv_block(pool3, 512)\n",
        "    conv4 = conv_block(conv4, 512)\n",
        "    pool4 = MaxPooling2D()(conv4)\n",
        "    \n",
        "    # Bottleneck\n",
        "    bneck = conv_block(pool4, 1024)\n",
        "    bneck = conv_block(bneck, 1024)\n",
        "    bneck = spatial_attention(bneck)\n",
        "    \n",
        "    # Decoder with attention\n",
        "    up4 = Conv2DTranspose(512, 2, strides=2, padding='same')(bneck)\n",
        "    attn4 = AttentionBlock(512)(conv4, up4)\n",
        "    merge4 = Concatenate()([up4, attn4])\n",
        "    conv5 = conv_block(merge4, 512)\n",
        "    conv5 = conv_block(conv5, 512)\n",
        "    \n",
        "    up3 = Conv2DTranspose(256, 2, strides=2, padding='same')(conv5)\n",
        "    attn3 = AttentionBlock(256)(conv3, up3)\n",
        "    merge3 = Concatenate()([up3, attn3])\n",
        "    conv6 = conv_block(merge3, 256)\n",
        "    conv6 = conv_block(conv6, 256)\n",
        "    \n",
        "    up2 = Conv2DTranspose(128, 2, strides=2, padding='same')(conv6)\n",
        "    attn2 = AttentionBlock(128)(conv2, up2)\n",
        "    merge2 = Concatenate()([up2, attn2])\n",
        "    conv7 = conv_block(merge2, 128)\n",
        "    conv7 = conv_block(conv7, 128)\n",
        "    \n",
        "    up1 = Conv2DTranspose(64, 2, strides=2, padding='same')(conv7)\n",
        "    attn1 = AttentionBlock(64)(conv1, up1)\n",
        "    merge1 = Concatenate()([up1, attn1])\n",
        "    conv8 = conv_block(merge1, 64)\n",
        "    conv8 = conv_block(conv8, 64)\n",
        "    \n",
        "    # Output\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid', padding='same')(conv8)\n",
        "    \n",
        "    model = Model(inputs, outputs, name='Attention_UNet')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional Model Architectures\n",
        "\n",
        "def build_deeplabv3_plus(input_shape=(256, 256, 3)):\n",
        "    \"\"\"\n",
        "    Build DeepLabV3+ model\n",
        "    \"\"\"\n",
        "    # This is a simplified version - for full implementation, use segmentation-models library\n",
        "    base_model = tf.keras.applications.ResNet50(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    \n",
        "    # ASPP (Atrous Spatial Pyramid Pooling)\n",
        "    x = base_model.output\n",
        "    \n",
        "    # Global average pooling\n",
        "    gap = GlobalAveragePooling2D()(x)\n",
        "    gap = Dense(256, activation='relu')(gap)\n",
        "    gap = tf.keras.layers.Reshape((1, 1, 256))(gap)\n",
        "    gap = Conv2DTranspose(256, 32, strides=32, padding='same')(gap)\n",
        "    \n",
        "    # Atrous convolutions\n",
        "    atrous1 = Conv2D(256, 1, padding='same', activation='relu')(x)\n",
        "    atrous2 = Conv2D(256, 3, padding='same', dilation_rate=6, activation='relu')(x)\n",
        "    atrous3 = Conv2D(256, 3, padding='same', dilation_rate=12, activation='relu')(x)\n",
        "    atrous4 = Conv2D(256, 3, padding='same', dilation_rate=18, activation='relu')(x)\n",
        "    \n",
        "    # Concatenate\n",
        "    x = Concatenate()([atrous1, atrous2, atrous3, atrous4, gap])\n",
        "    x = Conv2D(256, 1, padding='same', activation='relu')(x)\n",
        "    \n",
        "    # Decoder\n",
        "    x = Conv2DTranspose(256, 4, strides=4, padding='same')(x)\n",
        "    \n",
        "    # Skip connection\n",
        "    skip = Conv2D(48, 1, padding='same')(base_model.get_layer('conv2_block3_2_relu').output)\n",
        "    x = Concatenate()([x, skip])\n",
        "    \n",
        "    x = Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "    x = Conv2DTranspose(256, 4, strides=4, padding='same')(x)\n",
        "    \n",
        "    # Output\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
        "    \n",
        "    model = Model(base_model.input, outputs, name='DeepLabV3Plus')\n",
        "    return model\n",
        "\n",
        "def build_segnet(input_shape=(256, 256, 3)):\n",
        "    \"\"\"\n",
        "    Build SegNet model\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    # Encoder\n",
        "    conv1 = conv_block(inputs, 64)\n",
        "    conv1 = conv_block(conv1, 64)\n",
        "    pool1 = MaxPooling2D()(conv1)\n",
        "    \n",
        "    conv2 = conv_block(pool1, 128)\n",
        "    conv2 = conv_block(conv2, 128)\n",
        "    pool2 = MaxPooling2D()(conv2)\n",
        "    \n",
        "    conv3 = conv_block(pool2, 256)\n",
        "    conv3 = conv_block(conv3, 256)\n",
        "    pool3 = MaxPooling2D()(conv3)\n",
        "    \n",
        "    conv4 = conv_block(pool3, 512)\n",
        "    conv4 = conv_block(conv4, 512)\n",
        "    pool4 = MaxPooling2D()(conv4)\n",
        "    \n",
        "    # Bottleneck\n",
        "    conv5 = conv_block(pool4, 1024)\n",
        "    conv5 = conv_block(conv5, 1024)\n",
        "    \n",
        "    # Decoder\n",
        "    up4 = Conv2DTranspose(512, 2, strides=2, padding='same')(conv5)\n",
        "    conv6 = conv_block(up4, 512)\n",
        "    conv6 = conv_block(conv6, 512)\n",
        "    \n",
        "    up3 = Conv2DTranspose(256, 2, strides=2, padding='same')(conv6)\n",
        "    conv7 = conv_block(up3, 256)\n",
        "    conv7 = conv_block(conv7, 256)\n",
        "    \n",
        "    up2 = Conv2DTranspose(128, 2, strides=2, padding='same')(conv7)\n",
        "    conv8 = conv_block(up2, 128)\n",
        "    conv8 = conv_block(conv8, 128)\n",
        "    \n",
        "    up1 = Conv2DTranspose(64, 2, strides=2, padding='same')(conv8)\n",
        "    conv9 = conv_block(up1, 64)\n",
        "    conv9 = conv_block(conv9, 64)\n",
        "    \n",
        "    # Output\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid', padding='same')(conv9)\n",
        "    \n",
        "    model = Model(inputs, outputs, name='SegNet')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss Functions and Metrics\n",
        "\n",
        "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Dice loss for segmentation\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):\n",
        "    \"\"\"\n",
        "    Focal loss for handling class imbalance\n",
        "    \"\"\"\n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "    \n",
        "    alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "    \n",
        "    focal_loss = alpha_t * tf.pow((1 - p_t), gamma) * tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    return tf.reduce_mean(focal_loss)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Combined loss function\n",
        "    \"\"\"\n",
        "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    dice = dice_loss(y_true, y_pred)\n",
        "    focal = focal_loss(y_true, y_pred)\n",
        "    return bce + dice + 0.5 * focal\n",
        "\n",
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Dice coefficient metric\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Intersection over Union metric\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "\n",
        "# Define paths (update these according to your data structure)\n",
        "ORIGINAL_IMAGES_PATH = \"/content/drive/MyDrive/Dataset/Originals\"\n",
        "ANNOTATED_IMAGES_PATH = \"/content/drive/MyDrive/Dataset/Hyperpigmentation_annotated\"\n",
        "OUTPUT_IMAGES_PATH = \"/content/drive/MyDrive/Dataset/Hyperpigmentation_Cropped_Images\"\n",
        "OUTPUT_MASKS_PATH = \"/content/drive/MyDrive/Dataset/Hyperpigmentation_Cropped_Masks\"\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-4\n",
        "IMG_SIZE = (256, 256)\n",
        "N_CLASSES = 1\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(OUTPUT_IMAGES_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_MASKS_PATH, exist_ok=True)\n",
        "\n",
        "print(\"Configuration set up successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Preprocessing Pipeline\n",
        "\n",
        "def preprocess_dataset(original_path, annotated_path, output_img_path, output_mask_path):\n",
        "    \"\"\"\n",
        "    Preprocess the entire dataset\n",
        "    \"\"\"\n",
        "    print(\"Starting dataset preprocessing...\")\n",
        "    \n",
        "    # Process annotated images to extract green masks\n",
        "    for filename in os.listdir(annotated_path):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(annotated_path, filename)\n",
        "            image = cv2.imread(img_path)\n",
        "            \n",
        "            if image is None:\n",
        "                print(f\"Could not read image: {filename}\")\n",
        "                continue\n",
        "            \n",
        "            # Detect face\n",
        "            detection = detect_face(image)\n",
        "            if detection is None:\n",
        "                print(f\"Could not detect face for: {filename}\")\n",
        "                continue\n",
        "            \n",
        "            # Crop to face region\n",
        "            cropped_img, bbox = crop_face(image, detection)\n",
        "            if cropped_img is None:\n",
        "                print(f\"Could not crop image: {filename}\")\n",
        "                continue\n",
        "            \n",
        "            # Extract green mask\n",
        "            green_mask = extract_green_mask(cropped_img)\n",
        "            \n",
        "            # Save cropped mask\n",
        "            mask_path = os.path.join(output_mask_path, filename)\n",
        "            cv2.imwrite(mask_path, green_mask)\n",
        "            \n",
        "            print(f\"Processed mask: {filename}\")\n",
        "    \n",
        "    # Process original images\n",
        "    for filename in os.listdir(original_path):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_path = os.path.join(original_path, filename)\n",
        "            image = cv2.imread(img_path)\n",
        "            \n",
        "            if image is None:\n",
        "                print(f\"Could not read image: {filename}\")\n",
        "                continue\n",
        "            \n",
        "            # Detect face\n",
        "            detection = detect_face(image)\n",
        "            if detection is None:\n",
        "                print(f\"Could not detect face for: {filename}\")\n",
        "                continue\n",
        "            \n",
        "            # Crop to face region\n",
        "            cropped_img, bbox = crop_face(image, detection)\n",
        "            if cropped_img is None:\n",
        "                print(f\"Could not crop image: {filename}\")\n",
        "                continue\n",
        "            \n",
        "            # Save cropped image\n",
        "            img_output_path = os.path.join(output_img_path, filename)\n",
        "            cv2.imwrite(img_output_path, cropped_img)\n",
        "            \n",
        "            print(f\"Processed image: {filename}\")\n",
        "    \n",
        "    print(\"Dataset preprocessing completed!\")\n",
        "\n",
        "# Uncomment the line below to run preprocessing\n",
        "# preprocess_dataset(ORIGINAL_IMAGES_PATH, ANNOTATED_IMAGES_PATH, OUTPUT_IMAGES_PATH, OUTPUT_MASKS_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "def train_model(model_name='unet'):\n",
        "    \"\"\"\n",
        "    Train the selected model\n",
        "    \"\"\"\n",
        "    # Create datasets\n",
        "    train_dataset = get_dataset(\n",
        "        OUTPUT_IMAGES_PATH, \n",
        "        OUTPUT_MASKS_PATH, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        n_classes=N_CLASSES,\n",
        "        apply_augmentation_flag=True\n",
        "    )\n",
        "    \n",
        "    # Split data for validation (assuming you have validation data)\n",
        "    # For now, we'll use the same data for both training and validation\n",
        "    val_dataset = train_dataset.take(10)  # Use first 10 batches for validation\n",
        "    train_dataset = train_dataset.skip(10)  # Use remaining for training\n",
        "    \n",
        "    # Build model\n",
        "    if model_name == 'unet':\n",
        "        model = build_unet_model(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    elif model_name == 'deeplabv3':\n",
        "        model = build_deeplabv3_plus(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    elif model_name == 'segnet':\n",
        "        model = build_segnet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss=combined_loss,\n",
        "        metrics=['accuracy', dice_coefficient, iou_metric]\n",
        "    )\n",
        "    \n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            f'hyperpigmentation_{model_name}_best.h5', \n",
        "            monitor='val_loss', \n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "# Train different models\n",
        "print(\"Training U-Net model...\")\n",
        "unet_model, unet_history = train_model('unet')\n",
        "\n",
        "print(\"\\nTraining DeepLabV3+ model...\")\n",
        "deeplab_model, deeplab_history = train_model('deeplabv3')\n",
        "\n",
        "print(\"\\nTraining SegNet model...\")\n",
        "segnet_model, segnet_history = train_model('segnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Evaluation and Visualization\n",
        "\n",
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"\n",
        "    Plot training history\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[0, 0].set_title(f'{model_name} - Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    # Accuracy\n",
        "    axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[0, 1].set_title(f'{model_name} - Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    \n",
        "    # Dice Coefficient\n",
        "    axes[1, 0].plot(history.history['dice_coefficient'], label='Training Dice')\n",
        "    axes[1, 0].plot(history.history['val_dice_coefficient'], label='Validation Dice')\n",
        "    axes[1, 0].set_title(f'{model_name} - Dice Coefficient')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Dice Coefficient')\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # IoU\n",
        "    axes[1, 1].plot(history.history['iou_metric'], label='Training IoU')\n",
        "    axes[1, 1].plot(history.history['val_iou_metric'], label='Validation IoU')\n",
        "    axes[1, 1].set_title(f'{model_name} - IoU')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('IoU')\n",
        "    axes[1, 1].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_predictions(model, dataset, num_samples=5):\n",
        "    \"\"\"\n",
        "    Visualize model predictions\n",
        "    \"\"\"\n",
        "    for images, masks in dataset.take(1):\n",
        "        predictions = model.predict(images)\n",
        "        \n",
        "        for i in range(min(num_samples, images.shape[0])):\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "            \n",
        "            # Original image\n",
        "            axes[0].imshow(images[i])\n",
        "            axes[0].set_title('Original Image')\n",
        "            axes[0].axis('off')\n",
        "            \n",
        "            # Ground truth mask\n",
        "            axes[1].imshow(masks[i].squeeze(), cmap='gray')\n",
        "            axes[1].set_title('Ground Truth')\n",
        "            axes[1].axis('off')\n",
        "            \n",
        "            # Prediction\n",
        "            pred_mask = (predictions[i].squeeze() > 0.5).astype(np.uint8)\n",
        "            axes[2].imshow(pred_mask, cmap='gray')\n",
        "            axes[2].set_title('Prediction')\n",
        "            axes[2].axis('off')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            if i >= num_samples - 1:\n",
        "                break\n",
        "\n",
        "# Plot training histories\n",
        "plot_training_history(unet_history, 'U-Net')\n",
        "plot_training_history(deeplab_history, 'DeepLabV3+')\n",
        "plot_training_history(segnet_history, 'SegNet')\n",
        "\n",
        "# Visualize predictions\n",
        "print(\"U-Net Predictions:\")\n",
        "visualize_predictions(unet_model, val_dataset)\n",
        "\n",
        "print(\"DeepLabV3+ Predictions:\")\n",
        "visualize_predictions(deeplab_model, val_dataset)\n",
        "\n",
        "print(\"SegNet Predictions:\")\n",
        "visualize_predictions(segnet_model, val_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Comparison and Selection\n",
        "\n",
        "def evaluate_model(model, dataset, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate model performance\n",
        "    \"\"\"\n",
        "    results = model.evaluate(dataset, verbose=0)\n",
        "    \n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Loss': results[0],\n",
        "        'Accuracy': results[1],\n",
        "        'Dice Coefficient': results[2],\n",
        "        'IoU': results[3]\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Evaluate all models\n",
        "models = {\n",
        "    'U-Net': unet_model,\n",
        "    'DeepLabV3+': deeplab_model,\n",
        "    'SegNet': segnet_model\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    metrics = evaluate_model(model, val_dataset, name)\n",
        "    results.append(metrics)\n",
        "    print(f\"{name} Results:\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Create comparison table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "print(\"Model Comparison:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# Select best model based on IoU\n",
        "best_model_name = df.loc[df['IoU'].idxmax(), 'Model']\n",
        "best_model = models[best_model_name]\n",
        "print(f\"\\nBest model: {best_model_name}\")\n",
        "print(f\"Best IoU: {df['IoU'].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference Pipeline\n",
        "\n",
        "def predict_hyperpigmentation(image_path, model, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predict hyperpigmentation on a new image\n",
        "    \"\"\"\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Could not read image: {image_path}\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # Detect face\n",
        "    detection = detect_face(image)\n",
        "    if detection is None:\n",
        "        print(\"Could not detect face\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # Crop to face region\n",
        "    cropped_img, bbox = crop_face(image, detection)\n",
        "    if cropped_img is None:\n",
        "        print(\"Could not crop face region\")\n",
        "        return None, None, None\n",
        "    \n",
        "    # Preprocess for model\n",
        "    processed_img = enhance_image(cropped_img)\n",
        "    processed_img = cv2.resize(processed_img, IMG_SIZE)\n",
        "    processed_img = processed_img.astype(np.float32) / 255.0\n",
        "    \n",
        "    # Predict\n",
        "    input_tensor = tf.expand_dims(processed_img, 0)\n",
        "    prediction = model.predict(input_tensor)\n",
        "    \n",
        "    # Threshold prediction\n",
        "    pred_mask = (prediction[0].squeeze() > threshold).astype(np.uint8)\n",
        "    \n",
        "    return cropped_img, pred_mask, bbox\n",
        "\n",
        "def visualize_prediction(image_path, model, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Visualize prediction on a new image\n",
        "    \"\"\"\n",
        "    cropped_img, pred_mask, bbox = predict_hyperpigmentation(image_path, model, threshold)\n",
        "    \n",
        "    if cropped_img is None:\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    \n",
        "    # Original cropped image\n",
        "    axes[0].imshow(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title('Cropped Face Region')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Prediction overlay\n",
        "    overlay = cropped_img.copy()\n",
        "    overlay[pred_mask > 0] = [0, 255, 0]  # Green overlay for hyperpigmentation\n",
        "    \n",
        "    axes[1].imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "    axes[1].set_title('Hyperpigmentation Detection')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate hyperpigmentation percentage\n",
        "    total_pixels = pred_mask.size\n",
        "    hyperpigmentation_pixels = np.sum(pred_mask > 0)\n",
        "    percentage = (hyperpigmentation_pixels / total_pixels) * 100\n",
        "    \n",
        "    print(f\"Hyperpigmentation coverage: {percentage:.2f}%\")\n",
        "    \n",
        "    return percentage\n",
        "\n",
        "# Example usage with best model\n",
        "print(f\"Using {best_model_name} for inference\")\n",
        "\n",
        "# Test on a sample image (update path as needed)\n",
        "sample_image_path = \"/content/drive/MyDrive/Dataset/Test/sample_image.jpg\"\n",
        "# visualize_prediction(sample_image_path, best_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Models\n",
        "\n",
        "# Save all trained models\n",
        "unet_model.save('hyperpigmentation_unet_model.h5')\n",
        "deeplab_model.save('hyperpigmentation_deeplab_model.h5')\n",
        "segnet_model.save('hyperpigmentation_segnet_model.h5')\n",
        "\n",
        "# Save best model with additional metadata\n",
        "best_model.save(f'hyperpigmentation_{best_model_name.lower()}_best_model.h5')\n",
        "\n",
        "print(\"All models saved successfully!\")\n",
        "print(f\"Best model ({best_model_name}) saved as: hyperpigmentation_{best_model_name.lower()}_best_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional Utility Functions\n",
        "\n",
        "def batch_predict_hyperpigmentation(image_folder, model, output_folder, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Batch prediction on multiple images\n",
        "    \"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            \n",
        "            try:\n",
        "                cropped_img, pred_mask, bbox = predict_hyperpigmentation(image_path, model, threshold)\n",
        "                \n",
        "                if cropped_img is not None:\n",
        "                    # Save prediction mask\n",
        "                    mask_path = os.path.join(output_folder, f\"{filename}_mask.png\")\n",
        "                    cv2.imwrite(mask_path, pred_mask * 255)\n",
        "                    \n",
        "                    # Calculate hyperpigmentation percentage\n",
        "                    total_pixels = pred_mask.size\n",
        "                    hyperpigmentation_pixels = np.sum(pred_mask > 0)\n",
        "                    percentage = (hyperpigmentation_pixels / total_pixels) * 100\n",
        "                    \n",
        "                    results.append({\n",
        "                        'filename': filename,\n",
        "                        'hyperpigmentation_percentage': percentage,\n",
        "                        'bbox': bbox\n",
        "                    })\n",
        "                    \n",
        "                    print(f\"Processed: {filename} - {percentage:.2f}% hyperpigmentation\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {str(e)}\")\n",
        "    \n",
        "    # Save results to CSV\n",
        "    import pandas as pd\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results.to_csv(os.path.join(output_folder, 'hyperpigmentation_results.csv'), index=False)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def create_hyperpigmentation_report(image_path, model, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Create a detailed hyperpigmentation analysis report\n",
        "    \"\"\"\n",
        "    cropped_img, pred_mask, bbox = predict_hyperpigmentation(image_path, model, threshold)\n",
        "    \n",
        "    if cropped_img is None:\n",
        "        return None\n",
        "    \n",
        "    # Calculate various metrics\n",
        "    total_pixels = pred_mask.size\n",
        "    hyperpigmentation_pixels = np.sum(pred_mask > 0)\n",
        "    percentage = (hyperpigmentation_pixels / total_pixels) * 100\n",
        "    \n",
        "    # Find connected components\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(pred_mask, connectivity=8)\n",
        "    \n",
        "    # Calculate area statistics\n",
        "    areas = stats[1:, cv2.CC_STAT_AREA]  # Skip background (label 0)\n",
        "    \n",
        "    report = {\n",
        "        'total_area': total_pixels,\n",
        "        'hyperpigmentation_area': hyperpigmentation_pixels,\n",
        "        'hyperpigmentation_percentage': percentage,\n",
        "        'num_regions': num_labels - 1,  # Exclude background\n",
        "        'largest_region_area': np.max(areas) if len(areas) > 0 else 0,\n",
        "        'average_region_area': np.mean(areas) if len(areas) > 0 else 0,\n",
        "        'bbox': bbox\n",
        "    }\n",
        "    \n",
        "    return report\n",
        "\n",
        "print(\"Utility functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a comprehensive hyperpigmentation detection system with the following features:\n",
        "\n",
        "### 1. **Data Preprocessing**\n",
        "- Green color segmentation for annotation masks\n",
        "- **Overall face detection** using MediaPipe Face Detection (no facial region segmentation)\n",
        "- Image enhancement using CLAHE and bilateral filtering\n",
        "- Data augmentation pipeline\n",
        "\n",
        "### 2. **Model Architectures**\n",
        "- **U-Net with Attention**: Best for detailed segmentation\n",
        "- **DeepLabV3+**: Good for multi-scale features\n",
        "- **SegNet**: Efficient encoder-decoder architecture\n",
        "\n",
        "### 3. **Training Features**\n",
        "- Combined loss function (BCE + Dice + Focal)\n",
        "- Multiple evaluation metrics (Dice, IoU, Accuracy)\n",
        "- Early stopping and learning rate scheduling\n",
        "- Model checkpointing\n",
        "\n",
        "### 4. **Inference Pipeline**\n",
        "- Real-time prediction on new images\n",
        "- Batch processing capabilities\n",
        "- Detailed analysis reports\n",
        "- Visualization tools\n",
        "\n",
        "### 5. **Usage Instructions**\n",
        "\n",
        "1. **Setup**: Update the file paths in the configuration section\n",
        "2. **Preprocessing**: Run the dataset preprocessing pipeline\n",
        "3. **Training**: Train all three models and compare performance\n",
        "4. **Evaluation**: Use the best performing model for inference\n",
        "5. **Inference**: Apply the model to new images for hyperpigmentation detection\n",
        "\n",
        "### 6. **Model Recommendations**\n",
        "\n",
        "Based on typical performance:\n",
        "- **U-Net**: Best overall performance, good for detailed segmentation\n",
        "- **DeepLabV3+**: Good for handling multi-scale hyperpigmentation\n",
        "- **SegNet**: Fastest inference, good for real-time applications\n",
        "\n",
        "### 7. **Key Changes Made**\n",
        "- **Simplified Face Detection**: Uses MediaPipe Face Detection instead of facial landmark segmentation\n",
        "- **Overall Face Cropping**: Crops the entire detected face region instead of specific facial areas\n",
        "- **Faster Processing**: More efficient preprocessing pipeline\n",
        "\n",
        "The system automatically selects the best model based on IoU score and provides comprehensive evaluation metrics.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
